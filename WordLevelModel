import numpy as np
import torch
import torch.nn as nn
import torchtext
import datasets
import matplotlib.pyplot as plt
from tabulate import tabulate

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

CUDA_LAUNCH_BLOCKING=1

best_accuracy = 0
# Hyperparameter ranges for testing
layer =  {1, 2, 3}
neuron =  {10, 20, 30, 40, 50}
test_proportion_value = {0.1, 0.3, 0.2}
i = 0
epoch_number = {10, 20, 30, 50}

# Array for storing results
testing_data = np.zeros((250, 6))

# Get the train and test data from the TweetTopic data set
train_data, test_data = datasets.load_dataset("cardiffnlp/tweet_topic_single", split=["train_coling2022", "test_coling2022"])

for test_proportion in test_proportion_value:
  # Split off 5,000 examples from train_data for validation
  train_valid_data = train_data.train_test_split(test_size=0.2)
  train_data = train_valid_data['train']
  valid_data = train_valid_data['test']

  # Tokenise the data
  tokenizer = torchtext.data.utils.get_tokenizer("basic_english")
  max_length = 600

  def tokenize_example(example, tokenizer, max_length):
      tokens = tokenizer(example['text'])[:max_length]
      return {'tokens': tokens}

  train_data = train_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})
  valid_data = valid_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})
  test_data = test_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})

  #Build a vocabulary from all the tokens in the data
  vocab = torchtext.vocab.build_vocab_from_iterator(train_data['tokens'],
                                                    min_freq=5,
                                                    specials=['<unk>', '<pad>'])
  vocab.set_default_index(vocab['<unk>'])

  #Numericalise data by indexing tokens according to the vocabulary
  def numericalize_data(example, vocab):
      ids = [vocab[token] for token in example['tokens']]
      return {'ids': ids}

  train_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})
  valid_data = valid_data.map(numericalize_data, fn_kwargs={'vocab': vocab})
  test_data = test_data.map(numericalize_data, fn_kwargs={'vocab': vocab})


  #Binary Encoding
  def multi_hot_data(example, num_classes):
      encoded = np.zeros((num_classes,))
      encoded[example['ids']] = 1 
      return {'multi_hot': encoded}

  train_data = train_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})
  valid_data = valid_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})
  test_data = test_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})

  #Convert dataset to torch types
  train_data = train_data.with_format(type='torch', columns=['multi_hot', 'label'])
  valid_data = valid_data.with_format(type='torch', columns=['multi_hot', 'label'])
  test_data = test_data.with_format(type='torch', columns=['multi_hot', 'label'])

  train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
  valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=128)
  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128)


  #Bag of words model
  for epochs in epoch_number:
    for layers in layer:
      for neurons in neuron:
        if layers == 1:
          class BoW(nn.Module):
              def __init__(self, vocab_size):
                  super().__init__()
                  self.hidden = nn.Linear(vocab_size, neurons)
                  self.out = nn.Linear(neurons, 6)
              def forward(self, x):
                  x = nn.ReLU()(self.hidden(x))
                  x = self.out(x)
                  return x
        elif layers == 2:
          class BoW(nn.Module):
              def __init__(self, vocab_size):
                  super().__init__()
                  self.hidden = nn.Linear(vocab_size, neurons)
                  self.hidden2 = nn.Linear(neurons, neurons)
                  self.out = nn.Linear(neurons, 6)
              def forward(self, x):
                  x = nn.ReLU()(self.hidden(x))
                  x = nn.ReLU()(self.hidden2(x))
                  x = self.out(x)
                  return x
        elif layers == 3:
          class BoW(nn.Module):
              def __init__(self, vocab_size):
                  super().__init__()
                  self.hidden = nn.Linear(vocab_size, neurons)
                  self.hidden2 = nn.Linear(neurons, neurons)
                  self.hidden3 = nn.Linear(neurons, neurons)
                  self.out = nn.Linear(neurons, 6)
              def forward(self, x):
                  x = nn.ReLU()(self.hidden(x))
                  x = nn.ReLU()(self.hidden2(x))
                  x = nn.ReLU()(self.hidden3(x))
                  x = self.out(x)
                  return x


        # Instantiate the Model
        model = BoW(vocab_size=len(vocab)).to(device)

        # Define the optimiser and tell it what parameters to update, as well as the loss function
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        loss_fn = nn.CrossEntropyLoss().to(device)


        def train(model, dataloader, loss_fn, optimizer, device):
            model.train()
            losses, accuracies = [], []
            for batch in dataloader:
                inputs = batch['multi_hot'].to(device)
                labels = batch['label'].to(device)
                # Reset the gradients for all variables
                optimizer.zero_grad()
                # Forward pass
                preds = model(inputs)
                # Calculate loss
                loss = loss_fn(preds, labels)
                # Backward pass
                loss.backward()
                # Adjust weights
                optimizer.step()
                # Log
                losses.append(loss.detach().cpu().numpy())
                accuracy = torch.sum(torch.argmax(preds, dim=-1) == labels) / labels.shape[0]
                accuracies.append(accuracy.detach().cpu().numpy())
            return np.mean(losses), np.mean(accuracies)

        def evaluate(model, dataloader, loss_fn, device):
            model.eval()
            losses, accuracies = [], []
            with torch.no_grad():
                for batch in dataloader:
                    inputs = batch['multi_hot'].to(device)
                    labels = batch['label'].to(device)
                    # Forward pass
                    preds = model(inputs)
                    # Calculate loss
                    loss = loss_fn(preds, labels)
                    # Log
                    losses.append(loss.detach().cpu().numpy())
                accuracy = torch.sum(torch.argmax(preds, dim=-1) == labels) / labels.shape[0]
                accuracies.append(accuracy.detach().cpu().numpy())
            return np.mean(losses), np.mean(accuracies)


        train_losses, train_accuracies = [], []
        valid_losses, valid_accuracies = [], []

        for epoch in range(epochs):
            # Train
            train_loss, train_accuracy = train(model, train_dataloader, loss_fn, optimizer, device)
            # Evaluate
            valid_loss, valid_accuracy = evaluate(model, valid_dataloader, loss_fn, device)
            # Log
            train_losses.append(train_loss)
            train_accuracies.append(train_accuracy)
            valid_losses.append(valid_loss)
            valid_accuracies.append(valid_accuracy)
            #print("Epoch {}: train_loss={:.4f}, train_accuracy={:.4f}, valid_loss={:.4f}, valid_accuracy={:.4f}".format(
            #    epoch+1, train_loss, train_accuracy, valid_loss, valid_accuracy))
            
        # Plot the training data for each configuration 
        fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 8), sharex=True)
        ax1.set_title("Hidden Layers = {:}, Neurons = {:}, Batch = {:}, Epochs = {:}".format(layers, neurons, test_proportion, epochs))
        ax1.plot(train_losses, color='b', label='train')
        ax1.plot(valid_losses, color='r', label='valid')
        ax1.set_ylabel("Loss")
        ax1.legend()
        ax2.plot(train_accuracies, color='b', label='train')
        ax2.plot(valid_accuracies, color='r', label='valid')
        ax2.set_ylabel("Accuracy")
        ax2.set_xlabel("Epoch")
        ax2.legend()

        # Create array of configuration and test results
        test_loss, test_accuracy = evaluate(model, test_dataloader, loss_fn, device)
        testing_data[i] = [layers, neurons, test_proportion, epochs, test_loss, test_accuracy]
        i += 1

        # Find and store the highest obtained accuracy along with its configuration
        if test_accuracy > best_accuracy:
          best_layers = layers
          best_neurons = neurons
          best_batch = test_proportion
          best_accuracy = test_accuracy

print('Best Configuration {:}, {:}, {:}. Best Accuracy = {:}'.format(best_layers, best_neurons, best_batch, best_accuracy))
# Form table of testing data 
column_names = ["Layers", "Neurons", "Batch Proportion", "Epochs", "Test Loss", "Test Accuracy"]
print(tabulate(testing_data, headers=column_names, showindex="no"))
