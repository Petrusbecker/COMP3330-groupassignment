
from torchtext.data.utils import ngrams_iterator

# # In torch, return an iterator that yields the given tokens and their ngrams.
# token_list = ['here', 'we', 'are']
# print(list(ngrams_iterator(token_list, 3)))
# # Out:
# # ['here', 'we', 'are', 'here we', 'we are']

import numpy as np
import torch
import torch.nn as nn
import torchtext
import datasets
import matplotlib.pyplot as plt
from tabulate import tabulate
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

CUDA_LAUNCH_BLOCKING=1

# Get the train and test data from the TweetTopic data set
train_data, test_data = datasets.load_dataset("cardiffnlp/tweet_topic_single", split=["train_coling2022", "test_coling2022"])

best_accuracy = 0
# Hyperparameter ranges for testing
layer =  {1, 2, 3}
neuron =  {10, 20, 30, 40, 50}
test_proportion_value = {0.1, 0.3, 0.2}
ngrams = {2, 3}
epoch_number = {10, 20, 30, 50}
i = 0

testing_data = np.zeros((1000, 6))

# Split training and validation data according to the test batch proportion of 0.1 or 0.2
train_valid_data = train_data.train_test_split(test_size=0.1)
train_data = train_valid_data['train']
valid_data = train_valid_data['test']

# Form tokens

tokenizer = torchtext.data.utils.get_tokenizer("basic_english")
max_length = 600

def tokenize_example(example, tokenizer, max_length):
  tokens = tokenizer(example['text'])[:max_length]
  return {'tokens': tokens}


train_data = train_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})
valid_data = valid_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})
test_data = test_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})

# Form the two word tokens for the bigram model
for i in range(len(train_data)):
  train_data[i]['tokens'] = ngrams_iterator(train_data[i]['tokens'], 2)

for i in range(len(test_data)):
  test_data[i]['tokens'] = ngrams_iterator(test_data[i]['tokens'], 2)

for i in range(len(valid_data)):
  valid_data[i]['tokens'] = ngrams_iterator(valid_data[i]['tokens'], 2)

#Build a vocabulary from all the tokens in the data
vocab = torchtext.vocab.build_vocab_from_iterator(train_data['tokens'],
                                                min_freq=5,
                                                specials=['<unk>', '<pad>'])
vocab.set_default_index(vocab['<unk>'])

#Numericalise data by indexing tokens according to the vocabulary
def numericalize_data(example, vocab):
  ids = [vocab[token] for token in example['tokens']]
  return {'ids': ids}

train_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})
valid_data = valid_data.map(numericalize_data, fn_kwargs={'vocab': vocab})
test_data = test_data.map(numericalize_data, fn_kwargs={'vocab': vocab})


#Binary Encoding
def multi_hot_data(example, num_classes):
  encoded = np.zeros((num_classes,))
  encoded[example['ids']] = 1 
  return {'multi_hot': encoded}

train_data = train_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})
valid_data = valid_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})
test_data = test_data.map(multi_hot_data, fn_kwargs={'num_classes': len(vocab)})

#Convert dataset to torch types
train_data = train_data.with_format(type='torch', columns=['multi_hot', 'label'])
valid_data = valid_data.with_format(type='torch', columns=['multi_hot', 'label'])
test_data = test_data.with_format(type='torch', columns=['multi_hot', 'label'])

train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=128)
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128)


#Bag of words model
for layers in layer:
  for neurons in neuron:
    for epochs in epoch_number:
    if layers == 1:
      class BoW(nn.Module):
          def __init__(self, vocab_size):
              super().__init__()
              self.hidden = nn.Linear(vocab_size, neurons)
              self.out = nn.Linear(neurons, 6)
          def forward(self, x):
              x = nn.ReLU()(self.hidden(x))
              x = self.out(x)
              return x
    elif layers == 2:
      class BoW(nn.Module):
          def __init__(self, vocab_size):
              super().__init__()
              self.hidden = nn.Linear(vocab_size, neurons)
              self.hidden2 = nn.Linear(neurons, neurons)
              self.out = nn.Linear(neurons, 6)
          def forward(self, x):
              x = nn.ReLU()(self.hidden(x))
              x = nn.ReLU()(self.hidden2(x))
              x = self.out(x)
              return x
    elif layers == 3:
      class BoW(nn.Module):
          def __init__(self, vocab_size):
              super().__init__()
              self.hidden = nn.Linear(vocab_size, neurons)
              self.hidden2 = nn.Linear(neurons, neurons)
              self.hidden3 = nn.Linear(neurons, neurons)
              self.out = nn.Linear(neurons, 6)
          def forward(self, x):
              x = nn.ReLU()(self.hidden(x))
              x = nn.ReLU()(self.hidden2(x))
              x = nn.ReLU()(self.hidden3(x))
              x = self.out(x)
              return x


    # Instantiate the Model
    model = BoW(vocab_size=len(vocab)).to(device)

    # Define the optimiser and tell it what parameters to update, as well as the loss function
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss().to(device)


    def train(model, dataloader, loss_fn, optimizer, device):
        model.train()
        losses, accuracies = [], []
        for batch in dataloader:
            inputs = batch['multi_hot'].to(device)
            labels = batch['label'].to(device)
            # Reset the gradients for all variables
            optimizer.zero_grad()
            # Forward pass
            preds = model(inputs)
            # Calculate loss
            loss = loss_fn(preds, labels)
            # Backward pass
            loss.backward()
            # Adjust weights
            optimizer.step()
            # Log
            losses.append(loss.detach().cpu().numpy())
            accuracy = torch.sum(torch.argmax(preds, dim=-1) == labels) / labels.shape[0]
            accuracies.append(accuracy.detach().cpu().numpy())
        return np.mean(losses), np.mean(accuracies)

    def evaluate(model, dataloader, loss_fn, device):
        model.eval()
        losses, accuracies = [], []
        with torch.no_grad():
            for batch in dataloader:
                inputs = batch['multi_hot'].to(device)
                labels = batch['label'].to(device)
                # Forward pass
                preds = model(inputs)
                # Calculate loss
                loss = loss_fn(preds, labels)
                # Log
                losses.append(loss.detach().cpu().numpy())
            accuracy = torch.sum(torch.argmax(preds, dim=-1) == labels) / labels.shape[0]
            accuracies.append(accuracy.detach().cpu().numpy())
        return np.mean(losses), np.mean(accuracies)


    train_losses, train_accuracies = [], []
    valid_losses, valid_accuracies = [], []

    for epoch in range(epoch_number):
        # Train
        train_loss, train_accuracy = train(model, train_dataloader, loss_fn, optimizer, device)
        # Evaluate
        valid_loss, valid_accuracy = evaluate(model, valid_dataloader, loss_fn, device)
        # Log
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        valid_losses.append(valid_loss)
        valid_accuracies.append(valid_accuracy)
     
        
    # Print the training data graphs for each configuration    
    fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 8), sharex=True)
    ax1.set_title("Hidden Layers = {:}, Neurons = {:}, Batch = {:}".format(layers, neurons, 0.2))
    ax1.plot(train_losses, color='b', label='train')
    ax1.plot(valid_losses, color='r', label='valid')
    ax1.set_ylabel("Loss")
    ax1.legend()
    ax2.plot(train_accuracies, color='b', label='train')
    ax2.plot(valid_accuracies, color='r', label='valid')
    ax2.set_ylabel("Accuracy")
    ax2.set_xlabel("Epoch")
    ax2.legend()

    # Create array storing the test data for each configuration
    test_loss, test_accuracy = evaluate(model, test_dataloader, loss_fn, device)
    testing_data[i] = [layers, neurons, 0.2, epochs, test_loss, test_accuracy]
    i += 1
    
    # Store best accuracy and its configuration at the end of training
    if test_accuracy > best_accuracy:
      best_layers = layers
      best_neurons = neurons
      best_batch = 0.2
      best_accuracy = test_accuracy

    
     

        

print('Best Configuration {:}, {:}, {:}. Best Accuracy = {:}'.format(best_layers, best_neurons, best_batch, best_accuracy))
# Form table of testing data
column_names = ["Layers", "Neurons", "Batch Proportion", "Epochs", "Test Loss", "Test Accuracy"]
print(tabulate(testing_data, headers=column_names, showindex="no"))
